"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[4358],{85946:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>c,toc:()=>t});var a=s(85893),r=s(11151);const i={},o="Preprocessing of Categorical Data",c={id:"machine-learning/data-preprocess/categorical-data",title:"Preprocessing of Categorical Data",description:"In practice, collected data are often incomplete, inconsistent, and may contain many errors. Data Preprocessing is a data mining technology that processes raw data for further analysis.",source:"@site/docs/machine-learning/data-preprocess/categorical-data.md",sourceDirName:"machine-learning/data-preprocess",slug:"/machine-learning/data-preprocess/categorical-data",permalink:"/site/docs/machine-learning/data-preprocess/categorical-data",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"machinelearningSidebar",previous:{title:"Vectorization of logistic regression",permalink:"/site/docs/machine-learning/neural-network/logistic-regression-mathmatics-and-vectorization"},next:{title:"Feature Scaling",permalink:"/site/docs/machine-learning/data-preprocess/feature-scaling"}},l={},t=[{value:"Classification and ordering",id:"nominal-and-ordinal",level:2},{value:"Mapping of ordinal features",id:"mapping-of-ordinal-features",level:2},{value:"Encoding of class labels",id:"encoding-of-class-labels",level:2},{value:"One-hot encoding of nominal features",id:"onehot-encoding-of-nominal-features",level:2},{value:"The principle of one-hot encoding",id:"onehot-encoding-principle",level:3},{value:"Implementation of one-hot encoding",id:"onehot-encoding-implementation",level:3},{value:"Correlation of one-hot encoding",id:"onehot-encoding-correlation",level:3}];function d(e){const n={a:"a",annotation:"annotation",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"preprocessing-of-categorical-data",children:"Preprocessing of Categorical Data"}),"\n",(0,a.jsx)(n.p,{children:"In practice, collected data are often incomplete, inconsistent, and may contain many errors. Data Preprocessing is a data mining technology that processes raw data for further analysis."}),"\n",(0,a.jsx)(n.p,{children:"This article introduces the processing of Categorical Data."}),"\n",(0,a.jsx)(n.h2,{id:"nominal-and-ordinal",children:"Classification and ordering"}),"\n",(0,a.jsxs)(n.p,{children:["When processing categorical data, it is necessary to distinguish between ",(0,a.jsx)(n.strong,{children:"nominal"})," features and ",(0,a.jsx)(n.strong,{children:"ordinal"})," features."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Classification characteristics: Different categories, comparison with each other is meaningless. Such as name, gender, fruits, etc."}),"\n",(0,a.jsx)(n.li,{children:"Sequencing features: Different categories can be compared and sorted with each other. Such as very satisfied/generally satisfied/dissatisfied, small/medium/large, etc. Unlike numerical features, the difference between the two is generally meaningless."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The following ",(0,a.jsx)(n.code,{children:"df"})," variables represent some characteristics of the T-shirt:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> import pandas as pd\n>>> df = pd.DataFrame([\n... ['green', 'M', 10.1, 'class1'],\n... ['red', 'L', 13.5, 'class2'],\n... ['blue', 'XL', 15.3, 'class1']])\n>>> df.columns = ['color', 'size', 'price', 'classlabel']\n>>> df\n    color size price classlabel\n0 green M 10.1 class1\n1 red L 13.5 class2\n2 blue XL 15.3 class1\n"})}),"\n",(0,a.jsxs)(n.p,{children:["These include the categorical feature ",(0,a.jsx)(n.code,{children:"color"}),", the ordinal feature ",(0,a.jsx)(n.code,{children:"size"})," and the numeric feature ",(0,a.jsx)(n.code,{children:"price"}),". The last column is the classification category ",(0,a.jsx)(n.code,{children:"label"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"mapping-of-ordinal-features",children:"Mapping of ordinal features"}),"\n",(0,a.jsx)(n.p,{children:"In order to ensure that the learning algorithm can recognize the ordered features, it is necessary to manually map the classification string to an integer."}),"\n",(0,a.jsxs)(n.p,{children:["As for the T-shirt size in the above example, assuming that the order ",(0,a.jsxs)(n.span,{className:"katex",children:[(0,a.jsx)(n.span,{className:"katex-mathml",children:(0,a.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(n.semantics,{children:[(0,a.jsxs)(n.mrow,{children:[(0,a.jsx)(n.mi,{children:"X"}),(0,a.jsx)(n.mi,{children:"L"}),(0,a.jsx)(n.mo,{children:">"}),(0,a.jsx)(n.mi,{children:"L"}),(0,a.jsx)(n.mo,{children:">"}),(0,a.jsx)(n.mi,{children:"M"})]}),(0,a.jsx)(n.annotation,{encoding:"application/x-tex",children:"XL > L > M"})]})})}),(0,a.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.7224em",verticalAlign:"-0.0391em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"},children:"X"}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"L"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(n.span,{className:"mrel",children:">"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.7224em",verticalAlign:"-0.0391em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",children:"L"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,a.jsx)(n.span,{className:"mrel",children:">"}),(0,a.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,a.jsxs)(n.span,{className:"base",children:[(0,a.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,a.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"})]})]})]})," is known, the following conversion can be performed:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> size_mapping = {\n...'XL': 3,\n...'L': 2,\n...'M': 1}\n>>> df['size'] = df['size'].map(size_mapping)\n>>> df\n    color size price classlabel\n0 green 1 10.1 class1\n1 red 2 13.5 class2\n2 blue 3 15.3 class1\n"})}),"\n",(0,a.jsxs)(n.p,{children:["For reverse transformation, create a reverse dictionary and ",(0,a.jsx)(n.code,{children:"map"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> inv_size_mapping = {v: k for k, v in size_mapping.items()}\n>>> df['size'].map(inv_size_mapping)\n0 M\n1L\n2XL\nName: size, dtype: object\n"})}),"\n",(0,a.jsx)(n.h2,{id:"encoding-of-class-labels",children:"Encoding of class labels"}),"\n",(0,a.jsx)(n.p,{children:"Many machine learning libraries require the class label Encoding to be an integer value; although scikit-learn has integrated this processing mechanism by default, it is recommended to get into the habit of manual conversion."}),"\n",(0,a.jsx)(n.p,{children:"The numeric size of a class label has no meaning, so you can directly use an enumeration for label conversion:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> import numpy as np\n>>> class_mapping = {label:idx for idx,label in\n... enumerate(np.unique(df['classlabel']))}\n>>> class_mapping\n{'class1': 0, 'class2': 1}\n"})}),"\n",(0,a.jsx)(n.p,{children:"Encode class labels as integers:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> df['classlabel'] = df['classlabel'].map(class_mapping)\n>>> df\n    color size price classlabel\n0 green 1 10.1 0\n1 red 2 13.5 1\n2 blue 3 15.3 0\n"})}),"\n",(0,a.jsx)(n.p,{children:"Reverse conversion:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> inv_class_mapping = {v: k for k, v in class_mapping.items()}\n>>> df['classlabel'] = df['classlabel'].map(inv_class_mapping)\n>>> df\n    color size price classlabel\n0 green 1 10.1 class1\n1 red 2 13.5 class2\n2 blue 3 15.3 class1\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Class labels can be encoded as integers more easily via ",(0,a.jsx)(n.a,{href:"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html",children:"sklearn.preprocessing.LabelEncoder"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> from sklearn.preprocessing import LabelEncoder\n>>> class_le = LabelEncoder()\n>>> y = class_le.fit_transform(df['classlabel'].values)\n>>> y\narray([0, 1, 0])\n"})}),"\n",(0,a.jsx)(n.p,{children:"Reverse conversion:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> class_le.inverse_transform(y)\narray(['class1', 'class2', 'class1'], dtype=object)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"onehot-encoding-of-nominal-features",children:"One-hot encoding of nominal features"}),"\n",(0,a.jsx)(n.h3,{id:"onehot-encoding-principle",children:"The principle of one-hot encoding"}),"\n",(0,a.jsxs)(n.p,{children:["Before introducing ",(0,a.jsx)(n.strong,{children:"One-Hot Encoding"}),", let me first explain why the encoding method in the previous chapter is not used."]}),"\n",(0,a.jsx)(n.p,{children:"If you encode it as before:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> X = df[['color', 'size', 'price']].values\n>>> color_le = LabelEncoder()\n>>> X[:, 0] = color_le.fit_transform(X[:, 0])\n>>>X\narray([[1, 1, 10.1],\n        [2, 2, 13.5],\n        [0, 3, 15.3]], dtype=object)\n"})}),"\n",(0,a.jsx)(n.p,{children:"The encoding result is:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"blue = 0"}),"\n",(0,a.jsx)(n.li,{children:"green = 1"}),"\n",(0,a.jsx)(n.li,{children:"red = 2"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["If you feed the above data to a classifier, one of the most common mistakes in dealing with categorical data occurs: although we know that the numbers 0, 1, and 2 do not represent magnitudes, the algorithm does not know that. Therefore, during the learning process, the algorithm will associate them by default, that is, assuming ",(0,a.jsx)(n.code,{children:"red > green > blue"}),". After this processing, the algorithm can still produce certain results, but its performance will be affected."]}),"\n",(0,a.jsxs)(n.p,{children:["The idea of one-hot encoding is to create a new feature for each value. For the above example, the color features can be converted into three new features: ",(0,a.jsx)(n.code,{children:"blue"}),", ",(0,a.jsx)(n.code,{children:"green"})," and ",(0,a.jsx)(n.code,{children:"red"}),", and then labeled with binary values. For the ",(0,a.jsx)(n.code,{children:"blue"})," sample, the encoding is ",(0,a.jsx)(n.code,{children:"blue=1"})," , ",(0,a.jsx)(n.code,{children:"green=0"})," , ",(0,a.jsx)(n.code,{children:"red=0"})," ."]}),"\n",(0,a.jsx)(n.h3,{id:"onehot-encoding-implementation",children:"Implementation of one-hot encoding"}),"\n",(0,a.jsxs)(n.p,{children:["Use ",(0,a.jsx)(n.a,{href:"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html",children:"sklearn.preprocessing.OneHotEncoder"})," to encode the feature ",(0,a.jsx)(n.code,{children:"color"})," and return a sparse matrix:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> from sklearn.preprocessing import OneHotEncoder\n>>> ohe = OneHotEncoder(categorical_features=[0])\n>>> ohe.fit_transform(X).toarray()\narray([[ 0. , 1. , 0. , 1. , 10.1],\n        [0., 0., 1., 2., 13.5],\n        [ 1. , 0. , 0. , 3. , 15.3]])\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Another more convenient one-hot encoding method is the ",(0,a.jsx)(n.code,{children:"get_dummies"})," method in pandas, which converts the specified string column of the DataFrame, leaving other columns unchanged:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> pd.get_dummies(df[['price', 'color', 'size']])\n    price size color_blue color_green color_red\n0 10.1 1 0 1 0\n1 13.5 2 0 0 1\n2 15.3 3 1 0 0\n"})}),"\n",(0,a.jsx)(n.h3,{id:"onehot-encoding-correlation",children:"Correlation of one-hot encoding"}),"\n",(0,a.jsxs)(n.p,{children:["When using popular coding data sets, you must remember that it introduces multicollinearity, that is, one variable can be linearly predicted by other variables (such as the matrix above, if ",(0,a.jsx)(n.code,{children:"blue"}),", ",(0,a.jsx)(n.code,{children:"green"}),", ",(0,a.jsx)(n.code,{children:"red"})," are known Any two of them can get the last one). This has an impact on certain operations such as matrix inversion."]}),"\n",(0,a.jsx)(n.p,{children:"To reduce the correlation between variables, we can simply remove a feature column from the one-hot encoding array."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"sklearn.preprocessing.OneHotEncoder"})," does not provide a feature column deletion method and needs to be converted into a numpy array before slicing:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> ohe = OneHotEncoder(categorical_features=[0])\n>>> ohe.fit_transform(X).toarray()[:, 1:]\narray([[ 1. , 0. , 1. , 10.1],\n[0., 1., 2., 13.5],\n[ 0. , 0. , 3. , 15.3]])\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"get_dummies"})," in pandas provides the parameter ",(0,a.jsx)(n.code,{children:"drop_first"}),", which can easily delete the first feature column:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:">>> pd.get_dummies(df[['price', 'color', 'size']],\n... drop_first=True)\n   price  size  color_green  color_red\n0   10.1     1            1          0\n1   13.5     2            0          1\n2   15.3     3            0          0\n"})})]})}function h(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},11151:(e,n,s)=>{s.d(n,{Z:()=>c,a:()=>o});var a=s(67294);const r={},i=a.createContext(r);function o(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);