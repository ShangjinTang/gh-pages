"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[7542],{78839:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>i,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>d,toc:()=>l});var r=t(74848),s=t(28453);const o={},a="\u4ee3\u7801\u5b9e\u73b0",d={id:"machine-learning/transformer/transformer-code-implementation",title:"\u4ee3\u7801\u5b9e\u73b0",description:"PyTorch \u4ee3\u7801\u5b9e\u73b0",source:"@site/docs/machine-learning/11-transformer/06-transformer-code-implementation.md",sourceDirName:"machine-learning/11-transformer",slug:"/machine-learning/transformer/transformer-code-implementation",permalink:"/docs/machine-learning/transformer/transformer-code-implementation",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:6,frontMatter:{},sidebar:"machinelearningSidebar",previous:{title:"\u5206\u5272\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236",permalink:"/docs/machine-learning/transformer/transformer-split-multi-head-attention"},next:{title:"Transformer \u7684\u53d8\u79cd",permalink:"/docs/machine-learning/transformer/transformer-variants"}},i={},l=[{value:"PyTorch \u4ee3\u7801\u5b9e\u73b0",id:"pytorch-\u4ee3\u7801\u5b9e\u73b0",level:2}];function _(n){const e={code:"code",h1:"h1",h2:"h2",pre:"pre",...(0,s.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.h1,{id:"\u4ee3\u7801\u5b9e\u73b0",children:"\u4ee3\u7801\u5b9e\u73b0"}),"\n",(0,r.jsx)(e.h2,{id:"pytorch-\u4ee3\u7801\u5b9e\u73b0",children:"PyTorch \u4ee3\u7801\u5b9e\u73b0"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nImplementation of Transformer: Attention Is All You Need\n\nReferences:\n    - [Paper arXiv: Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n    - [Blog Post: The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n    - [YouTube Video: The Illustrated Transformer](https://youtu.be/ISNdQcPhsts?si=VyFfVKoITGOV78OA)\n"""\n\nimport argparse\nimport math\nimport os\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchinfo import summary\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer("pe", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, : x.size(1)]\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        # Ensure that the model dimension (d_model) is divisible by the number of heads\n        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"\n        # Initialize dimensions\n        self.d_model = d_model  # Model\'s dimension\n        self.num_heads = num_heads  # Number of attention heads\n        self.d_k = (\n            d_model // num_heads\n        )  # Dimension of each head\'s key, query, and value\n        # Linear layers for transforming inputs\n        self.W_q = nn.Linear(d_model, d_model)  # Query transformation\n        self.W_k = nn.Linear(d_model, d_model)  # Key transformation\n        self.W_v = nn.Linear(d_model, d_model)  # Value transformation\n        self.W_o = nn.Linear(d_model, d_model)  # Output transformation\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Calculate attention scores\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        # Softmax is applied to obtain attention probabilities\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        # Multiply by values to obtain the final output\n        output = torch.matmul(attn_probs, V)\n        return output\n\n    def split_heads(self, x):\n        # Reshape the input to have num_heads for multi-head attention\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n\n    def combine_heads(self, x):\n        # Combine the multiple heads back to original shape\n        batch_size, _, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n\n    def forward(self, Q, K, V, mask=None):\n        # Apply linear transformations and split heads\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n        # Perform scaled dot-product attention\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        # Combine heads and apply output transformation\n        output = self.W_o(self.combine_heads(attn_output))\n        return output\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x\n\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x\n\n\nclass PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        src_vocab_size,\n        tgt_vocab_size,\n        d_model,\n        num_heads,\n        num_layers,\n        d_ff,\n        max_seq_length,\n        dropout,\n    ):\n        super(Transformer, self).__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n        self.encoder_layers = nn.ModuleList(\n            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n        )\n        self.decoder_layers = nn.ModuleList(\n            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n        )\n        self.fc = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def generate_mask(self, src, tgt):\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n        seq_length = tgt.size(1)\n        nopeak_mask = (\n            1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)\n        ).bool()\n        tgt_mask = tgt_mask & nopeak_mask\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n        src_embedded = self.dropout(\n            self.positional_encoding(self.encoder_embedding(src))\n        )\n        tgt_embedded = self.dropout(\n            self.positional_encoding(self.decoder_embedding(tgt))\n        )\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n\n        output = self.fc(dec_output)\n        return output\n\n\ndef main(args):\n    transformer = Transformer(\n        args.src_vocab_size,\n        args.tgt_vocab_size,\n        args.d_model,\n        args.num_heads,\n        args.num_layers,\n        args.d_ff,\n        args.max_seq_length,\n        args.dropout,\n    )\n\n    # Generate random sample data\n    src_data = torch.randint(\n        1, args.src_vocab_size, (64, args.max_seq_length)\n    )  # (batch_size, seq_length)\n    tgt_data = torch.randint(\n        1, args.tgt_vocab_size, (64, args.max_seq_length)\n    )  # (batch_size, seq_length)\n\n    print(summary(transformer, input_data=[src_data, tgt_data], device="cpu"))\n\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    optimizer = optim.Adam(\n        transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n    )\n\n    transformer.train()\n\n    for epoch in range(10):\n        optimizer.zero_grad()\n        output = transformer(src_data, tgt_data[:, :-1])\n        loss = criterion(\n            output.contiguous().view(-1, args.tgt_vocab_size),\n            tgt_data[:, 1:].contiguous().view(-1),\n        )\n        loss.backward()\n        optimizer.step()\n        print(f"Epoch: {epoch+1}, Loss: {loss.item()}")\n\n    transformer.eval()\n\n    # Generate random sample validation data\n    val_src_data = torch.randint(\n        1, args.src_vocab_size, (64, args.max_seq_length)\n    )  # (batch_size, seq_length)\n    val_tgt_data = torch.randint(\n        1, args.tgt_vocab_size, (64, args.max_seq_length)\n    )  # (batch_size, seq_length)\n\n    with torch.no_grad():\n        val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n        val_loss = criterion(\n            val_output.contiguous().view(-1, args.tgt_vocab_size),\n            val_tgt_data[:, 1:].contiguous().view(-1),\n        )\n        print(f"Validation Loss: {val_loss.item()}")\n\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\n        "--src_vocab_size", type=int, default=5000, help="source vocabulary size"\n    )\n    parser.add_argument(\n        "--tgt_vocab_size", type=int, default=5000, help="target vocabulary size"\n    )\n    parser.add_argument(\n        "--d_model",\n        type=int,\n        default=512,\n        help="dimensionality of model\'s input and output",\n    )\n    parser.add_argument(\n        "--num_heads",\n        type=int,\n        default=8,\n        help="number of attention heads in multi-head attention",\n    )\n    parser.add_argument(\n        "--num_layers",\n        type=int,\n        default=6,\n        help="number of layers for both the encoder and the decoder",\n    )\n    parser.add_argument(\n        "--d_ff",\n        type=int,\n        default=2048,\n        help="dimensionality of the inner layer in the feed-forward network",\n    )\n    parser.add_argument(\n        "--max_seq_length",\n        type=int,\n        default=100,\n        help="the maximum length of the sequence",\n    )\n    parser.add_argument(\n        "--dropout",\n        type=float,\n        default=0.1,\n        help="dropout rate for regularization",\n    )\n\n    try:\n        args = parser.parse_args()\n        main(args)\n    except KeyboardInterrupt:\n        print("Interrupted")\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\n'})})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(_,{...n})}):_(n)}},28453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>d});var r=t(96540);const s={},o=r.createContext(s);function a(n){const e=r.useContext(o);return r.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function d(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),r.createElement(o.Provider,{value:e},n.children)}}}]);