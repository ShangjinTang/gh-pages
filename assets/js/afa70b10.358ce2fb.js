"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[9809],{4420:e=>{e.exports=JSON.parse('{"label":"scikit-learn","permalink":"/site/docs/tags/scikit-learn","allTagsPath":"/site/docs/tags","count":16,"items":[{"id":"machine-learning/traditional-algorithms/bagging","title":"Bagging","description":"\u5957\u888b\u7b97\u6cd5\uff08Bagging\uff09\u662f\u4e00\u79cd\u96c6\u6210\u5b66\u4e60\u6280\u672f\uff0c\u4e0e MajorityVoteClassifier \u76f8\u4f3c\uff0c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e bagging \u4e0d\u662f\u6bcf\u6b21\u4f7f\u7528\u76f8\u540c\u7684\u8bad\u7ec3\u96c6\u6765\u62df\u5408\u5355\u4e2a\u5206\u7c7b\u5668\uff0c\u800c\u662f\u5bf9\u521d\u59cb\u8bad\u7ec3\u96c6\u8fdb\u884c bootstrap \u62bd\u6837\u3002\u8fd9\u4e5f\u662f bagging \u79f0\u4e3a\u5f15\u5bfc\u805a\u5408\uff08Bootstrap Aggregating\uff09\u7684\u539f\u56e0\u3002","permalink":"/site/docs/machine-learning/traditional-algorithms/bagging"},{"id":"machine-learning/evaluation-metrics/confusion-matrix","title":"Confusion Matrix","description":"In addition to accuracy, evaluation metrics also include other indicators, such as Precision, Recall, and F1 Score. They are evaluation metrics in confusion matrix.","permalink":"/site/docs/machine-learning/evaluation-metrics/confusion-matrix"},{"id":"machine-learning/data-preprocess/feature-scaling","title":"Feature Scaling","description":"Feature Scaling is a method of quantifying the values of different features into the same interval. It is also one of the key steps that is easily overlooked in preprocessing. With the exception of a few algorithms (such as decision trees and random forests), most machine learning and optimization algorithms perform better with feature scaling.","permalink":"/site/docs/machine-learning/data-preprocess/feature-scaling"},{"id":"machine-learning/model-selection/grid-search","title":"Grid Search","description":"Grid Search is a method of optimizing model performance by traversing a given combination of hyperparameters.","permalink":"/site/docs/machine-learning/model-selection/grid-search"},{"id":"machine-learning/traditional-algorithms/k-nearest-neighbors","title":"K-\u8fd1\u90bb","description":"\u672c\u6587\u4ecb\u7ecd\u61d2\u60f0\u5b66\u4e60\u7684\u4e00\u4e2a\u5178\u578b\u793a\u4f8b\uff1aK-\u8fd1\u90bb\uff08K-Nearest Neighbors, KNN\uff09\u7b97\u6cd5\u3002","permalink":"/site/docs/machine-learning/traditional-algorithms/k-nearest-neighbors"},{"id":"machine-learning/model-selection/learning-and-validation-curve","title":"Learning Curve and Validation Curve","description":"Drawing the learning curve and validation curve of the model is a commonly used debugging method. From it, you can intuitively see the performance of the model on the test set and verification set, and determine whether there are over-fitting or under-fitting problems. .","permalink":"/site/docs/machine-learning/model-selection/learning-and-validation-curve"},{"id":"machine-learning/feature-selection/feature-extraction-lda","title":"Linear discriminant analysis","description":"Linear Discriminant Analysis (LDA), as a feature extraction technology, can improve computational efficiency and prevent overfitting due to the curse of dimensionality. LDA is similar to PCA, except that the goal of PCA is to find the orthogonal component axes that maximize the variance in the data set, while the goal of LDA is to find the feature subspace that optimizes class separability.","permalink":"/site/docs/machine-learning/feature-selection/feature-extraction-lda"},{"id":"machine-learning/data-preprocess/categorical-data","title":"Preprocessing of Categorical Data","description":"In practice, collected data are often incomplete, inconsistent, and may contain many errors. Data Preprocessing is a data mining technology that processes raw data for further analysis.","permalink":"/site/docs/machine-learning/data-preprocess/categorical-data"},{"id":"machine-learning/feature-selection/feature-extraction-pca","title":"Principal Component Analysis","description":"Similar to feature selection, we can use feature extraction to reduce the number of features in the dataset. The difference is that feature selection preserves the original features, while feature extraction transforms or projects the data into a new feature space. This article introduces Principal Component Analysis (PCA), which is an unsupervised linear transformation technology whose main features are feature extraction and dimensionality reduction.","permalink":"/site/docs/machine-learning/feature-selection/feature-extraction-pca"},{"id":"machine-learning/evaluation-metrics/roc-and-auc","title":"ROC and AUC","description":"In signal detection theory, the Receiver Operating Characteristic (ROC) curve is a coordinate graphical analysis tool. The classification model can be evaluated by calculating the Area Under Curve (AUC) of the ROC. ROC analyzes a binary classification model.","permalink":"/site/docs/machine-learning/evaluation-metrics/roc-and-auc"},{"id":"machine-learning/neural-network/introduction-to-sklearn-with-perceptron","title":"Scikit-learn Intro: Using Perceptron","description":"The name Scikit-learn comes from SciKit (SciPy Toolkit), a third-party extension toolkit for SciPy. After years of development, Scikit-learn has become one of the most popular machine learning libraries. This article takes the classification of the Iris data set as an example to introduce it.","permalink":"/site/docs/machine-learning/neural-network/introduction-to-sklearn-with-perceptron"},{"id":"machine-learning/basics/start-with-iris-dataset","title":"Start with Iris Dataset","description":"Machine learning mainly includes preprocessing, training, evaluation and prediction stages. This article demostrates how to do a classification with scikit-learn.","permalink":"/site/docs/machine-learning/basics/start-with-iris-dataset"},{"id":"machine-learning/traditional-algorithms/ensemble-voting-classifer","title":"\u4f7f\u7528\u96c6\u5408\u7684\u6295\u7968\u5206\u7c7b\u65b9\u6cd5","description":"\u96c6\u5408\uff08Ensemble\uff09\u65b9\u6cd5\u7684\u76ee\u6807\u662f\u5c06\u4e0d\u540c\u7684\u5206\u7c7b\u5668\u7ec4\u5408\u5230\u4e00\u8d77\uff0c\u76f8\u6bd4\u6bcf\u4e2a\u5355\u72ec\u7684\u5206\u7c7b\u5668\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002","permalink":"/site/docs/machine-learning/traditional-algorithms/ensemble-voting-classifer"},{"id":"machine-learning/traditional-algorithms/decision-tree","title":"\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797","description":"\u51b3\u7b56\u6811\uff08Decision Tree\uff09\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u6811\u7ed3\u6784\uff0c\u4ee3\u8868\u7684\u662f\u6837\u672c\u7279\u5f81\u4e0e\u6837\u672c\u6807\u7b7e\u4e4b\u95f4\u7684\u4e00\u79cd\u6620\u5c04\u5173\u7cfb\u3002","permalink":"/site/docs/machine-learning/traditional-algorithms/decision-tree"},{"id":"machine-learning/traditional-algorithms/svm-nonlinear","title":"\u6838\u6280\u5de7\u548c\u975e\u7ebf\u6027 SVM","description":"\u5728\u539f\u59cb SVM \u53ea\u652f\u6301\u7ebf\u6027\u6837\u672c\u7684\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u6838\u6280\u5de7\uff08Kernel Trick\uff09\u53ef\u4ee5\u5bf9\u975e\u7ebf\u6027\u7684\u6837\u672c\u8fdb\u884c\u5206\u7c7b\u3002","permalink":"/site/docs/machine-learning/traditional-algorithms/svm-nonlinear"},{"id":"machine-learning/traditional-algorithms/svm-linear","title":"\u7ebf\u6027 SVM","description":"\u652f\u6301\u5411\u91cf\u673a\uff08Support Vector Machine, SVM\uff09\u662f\u4e00\u79cd\u76d1\u7763\u5f0f\u5b66\u4e60\u65b9\u6cd5\uff0c\u5176\u5b66\u4e60\u7b56\u7565\u4e3a\u6700\u5927\u5316\u6837\u672c\u4e0e\u8d85\u5e73\u9762\uff08\u5373\u51b3\u7b56\u8fb9\u754c\uff09\u4e4b\u95f4\u7684\u95f4\u9694\u3002\u672c\u6587\u4ecb\u7ecd\u7ebf\u6027 SVM \u7684\u539f\u7406\u53ca\u5b9e\u73b0\u3002","permalink":"/site/docs/machine-learning/traditional-algorithms/svm-linear"}],"unlisted":false}')}}]);