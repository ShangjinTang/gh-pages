<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-machinelearning/deeplearning-algorithms/lstm/AI-understanding-lstm" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.1">
<title data-rh="true">理解 LSTM 网络 | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ShangjinTang.github.io/site/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ShangjinTang.github.io/site/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ShangjinTang.github.io/site/docs/machinelearning/deeplearning-algorithms/lstm/AI-understanding-lstm"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="理解 LSTM 网络 | My Site"><meta data-rh="true" name="description" content="本文为 LSTM 的经典博客之一，原文链接为 Colah - Understanding LSTMs，译文链接为 Jey Zhang - 理解 LSTM 网络。"><meta data-rh="true" property="og:description" content="本文为 LSTM 的经典博客之一，原文链接为 Colah - Understanding LSTMs，译文链接为 Jey Zhang - 理解 LSTM 网络。"><link data-rh="true" rel="icon" href="/site/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ShangjinTang.github.io/site/docs/machinelearning/deeplearning-algorithms/lstm/AI-understanding-lstm"><link data-rh="true" rel="alternate" href="https://ShangjinTang.github.io/site/docs/machinelearning/deeplearning-algorithms/lstm/AI-understanding-lstm" hreflang="en"><link data-rh="true" rel="alternate" href="https://ShangjinTang.github.io/site/docs/machinelearning/deeplearning-algorithms/lstm/AI-understanding-lstm" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/site/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/site/blog/atom.xml" title="My Site Atom Feed"><link rel="stylesheet" href="/site/assets/css/styles.a8f65075.css">
<script src="/site/assets/js/runtime~main.cf54a81a.js" defer="defer"></script>
<script src="/site/assets/js/main.1d3c36fa.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/site/"><div class="navbar__logo"><img src="/site/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/site/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">My Site</b></a><a class="navbar__item navbar__link" href="/site/blog">Blog</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/site/docs/machinelearning/Certificates-deeplearning-ai">机器学习</a><a class="navbar__item navbar__link" href="/site/opengrok">OpenGrok</a><a class="navbar__item navbar__link" href="/site/docs/todo/">TODO</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/site/docs/docusaurus-mdx/mdx-features/intro">Docusaurus &amp; MDX</a><a href="https://github.com/ShangjinTang/site" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/site/docs/machinelearning/Certificates-deeplearning-ai">deeplearning.ai 课程证书</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/site/docs/machinelearning/concepts/AI-machine-learning-basic-types">concepts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/site/docs/machinelearning/data-preprocess/AI-data-preprocessing-categorical-data">data-preprocess</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/site/docs/machinelearning/deeplearning-algorithms/lstm/AI-understanding-lstm">deeplearning-algorithms</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/site/docs/machinelearning/deeplearning-algorithms/lstm/AI-understanding-lstm">lstm</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/site/docs/machinelearning/deeplearning-algorithms/lstm/AI-understanding-lstm">理解 LSTM 网络</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/site/docs/machinelearning/deeplearning-algorithms/neural-network/AI-mcp-neuron-and-perceptron">neural-network</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/site/docs/machinelearning/feature-selection/AI-feature-selection-feature-importance">feature-selection</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/site/docs/machinelearning/math-latex/latex-tutorial">math-latex</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/site/docs/machinelearning/metrics/AI-evaluation-metrics">metrics</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/site/docs/machinelearning/model-selection/AI-model-selection-k-fold-cross-validation">model-selection</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/site/docs/machinelearning/numpy-pandas/Python-cs231n-numpy-tutorial">numpy-pandas</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/site/docs/machinelearning/sklearn-pipeline/AI-sklearn-pipeline">sklearn-pipeline</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/site/docs/machinelearning/tensorflow-1/AI-tensorflow-introduction-and-installation">tensorflow-1</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/site/docs/machinelearning/traditional-algorithms/bagging/AI-bagging">traditional-algorithms</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/site/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">deeplearning-algorithms</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">lstm</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">理解 LSTM 网络</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>理解 LSTM 网络</h1></header><p>本文为 LSTM 的经典博客之一，原文链接为 <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener noreferrer">Colah - Understanding LSTMs</a>，译文链接为 <a href="http://www.jeyzhang.com/understanding-lstm-network.html" target="_blank" rel="noopener noreferrer">Jey Zhang - 理解 LSTM 网络</a>。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="循环神经网络">循环神经网络<a href="#循环神经网络" class="hash-link" aria-label="Direct link to 循环神经网络" title="Direct link to 循环神经网络">​</a></h2>
<p>人们的每次思考并不都是从零开始的。比如说你在阅读这篇文章时，你基于对前面的文字的理解来理解你目前阅读到的文字，而不是每读到一个文字时，都抛弃掉前面的思考，从头开始。你的记忆是有持久性的。</p>
<p>传统的神经网络并不能如此，这似乎是一个主要的缺点。例如，假设你在看一场电影，你想对电影里的每一个场景进行分类。传统的神经网络不能够基于前面的已分类场景来推断接下来的场景分类。</p>
<p>循环神经网络（Recurrent Neural Networks, RNN）解决了这个问题。这种神经网络带有环，可以将信息持久化。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" width="100" class="img_ev3q">
<p>在上图所示的神经网络 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span> 中，输入为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>，输出为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span> 上的环允许将每一步产生的信息传递到下一步中。</p>
<p>环的加入使得 RNN 变得神秘。不过，如果你多思考一下的话，其实 RNN 跟普通的神经网络也没有那么不同。一个 RNN 可以看作是同一个网络的多份副本，每一份都将信息传递到下一个副本。如果我们将环展开的话：</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" width="550" class="img_ev3q">
<p>这种链式结构展示了RNN与序列和列表的密切关系。RNN的这种结构能够非常自然地使用这类数据。而且事实的确如此。在过去的几年里，RNN 在一系列的任务中都取得了令人惊叹的成就，比如语音识别，语言建模，翻译，图片标题等等。关于 RNN 在各个领域所取得的令人惊叹的成就，参见 <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener noreferrer">这篇文章</a> 。</p>
<p>长短期记忆（Long Short Term Memory，LSTM）是这一系列成功中的必要组成部分。LSTM 是一种特殊的循环神经网络，在许多任务中，LSTM 表现得比标准的 RNN 要出色得多。几乎所有基于 RNN 的令人惊叹的结果都是 LSTM 取得的。本文接下来将着重介绍 LSTM。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="长期依赖问题">长期依赖问题<a href="#长期依赖问题" class="hash-link" aria-label="Direct link to 长期依赖问题" title="Direct link to 长期依赖问题">​</a></h2>
<p>RNN 的一个核心思想是将以前的信息连接到当前的任务中来，例如，通过前面的视频帧来帮助理解当前帧。如果 RNN 真的能够这样做的话，那么它们将会极其有用。但是事实真是如此吗？未必。</p>
<p>有时候，我们只需要看最近的信息，就可以完成当前的任务。比如，考虑一个语言模型，通过前面的单词来预测接下来的单词。如果我们想预测 句子 “the clouds are in the <em>sky</em>” 中的最后一个单词，我们不需要更多的上下文信息——很明显下一个单词应该是 sky。在这种情况下，当前位置与相关信息所在位置之间的距离相对较小，RNN 可以被训练来使用这样的信息。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" width="400" class="img_ev3q">
<p>然而，有时候我们需要更多的上下文信息。比如，我们想预测句子 “I grew up in France… I speak fluent <em>French</em>” 中的最后一个单词。最近的信息告诉我们，最后一个单词可能是某种语言的名字，然而如果我们想确定到底是哪种语言的话，我们需要 France 这个更远的上下文信息。实际上，相关信息和需要该信息的位置之间的距离可能非常的远。</p>
<p>不幸的是，随着距离的增大，RNN 对于如何将这样的信息连接起来无能为力。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" width="500" class="img_ev3q">
<p>理论上说，RNN 是有能力来处理这种长期依赖（Long Term Dependencies）的。人们可以通过精心调参来构建模型处理一个这种玩具问题（Toy Problem）。不过，在实际问题中，RNN并没有能力来学习这些。<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener noreferrer">Hochreiter (1991) German</a> 更深入地讲了这个问题，<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener noreferrer">Bengio, et al. (1994)</a> 发现了 RNN 的一些非常基础的问题。</p>
<p>幸运的是，LSTM 并没有上述问题！</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="lstm-网络">LSTM 网络<a href="#lstm-网络" class="hash-link" aria-label="Direct link to LSTM 网络" title="Direct link to LSTM 网络">​</a></h2>
<p>LSTM，全称为长短期记忆网络（Long Short Term Memory networks），是一种特殊的 RNN，能够学习到长期依赖关系。LSTM 由 <a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="noopener noreferrer">Hochreiter &amp; Schmidhuber (1997)</a> 提出，许多研究者进行了一系列的工作对其改进并使之发扬光大。LSTM 在许多问题上效果非常好，现在被广泛使用。</p>
<p>LSTM 在设计上明确地避免了长期依赖的问题。记住长期信息是小菜一碟！所有的循环神经网络都有着重复的神经网络模块形成链的形式。在普通的 RNN 中，重复模块结构非常简单，例如只有一个 tanh 层。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" width="700" class="img_ev3q">
<p>LSTM 也有这种链状结构，不过其重复模块的结构不同。LSTM的重复模块中有4个神经网络层，并且他们之间的交互非常特别。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" width="700" class="img_ev3q">
<p>现在暂且不必关心细节，稍候我们会一步一步地对LSTM的各个部分进行介绍。开始之前，我们先介绍一下将用到的标记。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" width="550" class="img_ev3q">
<p>在上图中，每条线表示向量的传递，从一个结点的输出传递到另外结点的输入。粉红圆表示向量的元素级操作，比如相加或者相乘。黄色方框表示神经网络的层。线合并表示向量的连接，线分叉表示向量复制。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="lstm-核心思想">LSTM 核心思想<a href="#lstm-核心思想" class="hash-link" aria-label="Direct link to LSTM 核心思想" title="Direct link to LSTM 核心思想">​</a></h2>
<p>LSTM 的关键是神经元状态（Cell State  ），下图中横穿整个神经元顶部的水平线。</p>
<p>神经元状态有点像是传送带，它直接穿过整个链，同时只有一些较小的线性交互。上面承载的信息可以很容易地流过而不改变。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" width="700" class="img_ev3q">
<p>LSTM 有能力对神经元状态添加或者删除信息，这种能力通过一种叫门的结构来控制。</p>
<p>门是一种选择性让信息通过的方法。它们由一个 Sigmoid 神经网络层和一个元素级相乘操作组成。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" width="100" class="img_ev3q">
<p>Sigmoid 层输出 0 到 1 之间的值，每个值表示对应的部分信息是否应该通过。0 值表示不允许信息通过，1 值表示让所有信息通过。一个 LSTM 有 3 个这种门，来保护和控制神经元状态。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="lstm-分步详解">LSTM 分步详解<a href="#lstm-分步详解" class="hash-link" aria-label="Direct link to LSTM 分步详解" title="Direct link to LSTM 分步详解">​</a></h2>
<p>LSTM 的第一步是决定我们将要从神经元状态中扔掉哪些信息。该决定由一个叫做遗忘门（Forget Gate）的 Sigmoid 层控制。遗忘门观察 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t−1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>，对于神经元状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">C_{t−1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span> 中的每一个元素，输出一个 0~1 之间的数。1 表示“完全保留该信息”，0 表示“完全丢弃该信息”。</p>
<p>回到之前的预测下一个单词的例子。在这样的一个问题中，神经元状态可能包含当前主语的性别信息，以用来选择正确的物主代词。当我们遇到一个新的主语时，我们就需要把旧的性别信息遗忘掉。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" width="700" class="img_ev3q">
<p>下一步是决定我们将会把哪些新信息存储到神经元状态中。这步分为两部分。首先，有一个叫做输入门（Input Gate）的 Sigmoid 层决定我们要更新哪些信息。接下来，一个 tanh 层创造了一个新的候选值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\tilde{C}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0702em;vertical-align:-0.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span><span style="top:-3.6023em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">~</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>，该值可能被加入到神经元状态中。在下一步中，我们将会把这两个值组合起来用于更新神经元状态。</p>
<p>在语言模型的例子中，我们可能想要把新主语的性别加到神经元状态中，来取代我们已经遗忘的旧值。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" width="700" class="img_ev3q">
<p>现在我们该更新旧神经元状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">C_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span> 到新状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">C_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 了。上面的步骤中已经决定了该怎么做，这一步我们只需要实际执行即可。</p>
<p>我们把旧状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">C_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span> 乘以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">f_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> ，忘掉我们已经决定忘记的内容。然后我们再加上 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>i</mi><mi>t</mi></msub><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">i_t \* \tilde{C}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1702em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord text" style="color:#cc0000"><span class="mord" style="color:#cc0000">\*</span></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span><span style="top:-3.6023em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">~</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>，这个值由新的候选值（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>C</mi><mo>~</mo></mover><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\tilde{C}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0702em;vertical-align:-0.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span><span style="top:-3.6023em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">~</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>）乘以候选值的每一个状态我们决定更新的程度（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>i</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">i_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>）构成。</p>
<p>还是语言模型的例子，在这一步，我们按照之前的决定，扔掉了旧的主语的性别信息，并且添加了新的信息。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" width="700" class="img_ev3q">
<p>最后，我们需要决定最终的输出。输出将会基于目前的神经元状态，并且会加入一些过滤。首先我们建立一个 Sigmoid 层的输出门（Output Gate），来决定我们将输出神经元的哪些部分。然后我们将神经元状态通过 tanh 之后（使得输出值在 -1 到 1 之间），与输出门相乘，这样我们只会输出我们想输出的部分。</p>
<p>对于语言模型的例子，由于刚刚只输出了一个主语，因此下一步可能需要输出与动词相关的信息。举例来说，可能需要输出主语是单数还是复数，以便于我们接下来选择动词时能够选择正确的形式。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" width="700" class="img_ev3q">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="lstm的变种">LSTM的变种<a href="#lstm的变种" class="hash-link" aria-label="Direct link to LSTM的变种" title="Direct link to LSTM的变种">​</a></h2>
<p>本文前面所介绍的 LSTM 是最普通的 LSTM，但并非所有的 LSTM 模型都与前面相同。事实上，似乎每一篇论文中所用到的 LSTM 都是稍微不一样的版本。不同之处很微小，不过其中一些值得介绍。</p>
<p>一个流行的 LSTM 变种，由 <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener noreferrer">Gers &amp; Schmidhuber (2000)</a> 提出，加入了“窥视孔连接（peephole connection）”。也就是说我们让各种门可以观察到神经元状态。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png" width="700" class="img_ev3q">
<p>上图中，对于所有的门都加入了“窥  视孔”，不过也有一些论文中只加一部分。</p>
<p>另一种变种是使用对偶的遗忘门和输入门。我们不再是单独地决定需要遗忘什么信息，需要加入什么新信息；而是一起做决定：我们只会在需要在某处放入新信息时忘记该处的旧值；我们只会在已经忘记旧值的位置放入新值。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png" width="700" class="img_ev3q">
<p>另一个变化更大一些的 LSTM 变种叫做 Gated Recurrent Unit，或者 GRU，由 <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener noreferrer">Cho, et al. (2014)</a> 提出。GRU 将遗忘门和输入门合并成为单一的“更新门(Update Gate)”。GRU 同时也将神经元状态（Cell State）和隐状态（Hidden State）合并，同时引入其他的一些变化。该模型比标准的 LSTM 模型更加简化，同时现在也变得越来越流行。</p>
<img loading="lazy" src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png" width="700" class="img_ev3q">
<p>另外还有很多其他的模型，比如 <a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="noopener noreferrer">Yao, et al. (2015)</a> 提出的 Depth Gated RNNs。同时，还有很多完全不同的解决长期依赖问题的方法，比如 <a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="noopener noreferrer">Koutnik, et al. (2014)</a> 提出的 Clockwork RNNs。</p>
<p>不同的模型中哪个最好？这其中的不同真的有关系吗？<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener noreferrer">Greff, et al. (2015)</a> 对流行的变种做了一个比较，发现它们基本相同。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="noopener noreferrer">Jozefowicz, et al. (2015)</a> 测试了一万多种 RNN 结构，发现其中的一些在特定的任务上效果比 LSTM   要好。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="总结">总结<a href="#总结" class="hash-link" aria-label="Direct link to 总结" title="Direct link to 总结">​</a></h2>
<p>前文中，我提到了人们使用 RNN 所取得的出色的成就。本质上，几乎所有的成就都是由 LSTM 取得的。对于大部分的任务，LSTM 表现得非常好。</p>
<p>由于 LSTM 写在纸上是一堆公式，因此看起来很吓人。希望本文的分步讲解能让读者更容易接受和理解。</p>
<p>LSTM 使得我们在使用 RNN 能完成的任务上迈进了一大步。很自然，我们会思考，还会有下一个一大步吗？研究工作者们的共同观点是：“是的！还有一个下一步，那就是注意力（Attention）！”注意力机制的思想是，在每一步中，都让 RNN 从一个更大的信息集合中去选择信息。举个例子，假如你使用 RNN 来生成一幅图片的说明文字，RNN 可能在输出每一个单词时，都会去观察图片的一部分。事实上，<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener noreferrer">Xu, et al.(2015)</a> 做的正是这个工作！如果你想探索注意力机制的话，这会是一个很有趣的起始点。现在已经有很多使用注意力的令人兴奋的成果，而且似乎更多的成果马上将会出来……</p>
<p>注意力并不是RNN研究中唯一让人兴奋的主题。举例说，由Kalchbrenner, et al. (2015)提出的Grid LSTM似乎极有前途。在生成式模型中使用 RNN 的工作——比如 <a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="noopener noreferrer">Gregor, et al. (2015)</a> 、<a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="noopener noreferrer">Chung, et al. (2015)</a> 以及 <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="noopener noreferrer">Bayer &amp; Osendorfer (2015)</a> ——看起来也非常有意思。最近的几年对于RNN来说是一段非常令人激动的 时间，接下来的几年也必将更加使人振奋！</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/site/docs/tags/machine-learning">Machine Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/site/docs/tags/lstm">LSTM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/site/docs/tags/rnn">RNN</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/site/docs/machinelearning/data-preprocess/AI-data-preprocessing-feature-scaling"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">特征缩放</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/site/docs/machinelearning/deeplearning-algorithms/neural-network/AI-mcp-neuron-and-perceptron"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">MCP 神经元和感知器</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#循环神经网络" class="table-of-contents__link toc-highlight">循环神经网络</a></li><li><a href="#长期依赖问题" class="table-of-contents__link toc-highlight">长期依赖问题</a></li><li><a href="#lstm-网络" class="table-of-contents__link toc-highlight">LSTM 网络</a></li><li><a href="#lstm-核心思想" class="table-of-contents__link toc-highlight">LSTM 核心思想</a></li><li><a href="#lstm-分步详解" class="table-of-contents__link toc-highlight">LSTM 分步详解</a></li><li><a href="#lstm的变种" class="table-of-contents__link toc-highlight">LSTM的变种</a></li><li><a href="#总结" class="table-of-contents__link toc-highlight">总结</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docusaurus &amp; MDX</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docusaurus.io/docs" target="_blank" rel="noopener noreferrer" class="footer__link-item">Docusaurus Official Docs<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://mdxjs.com/docs" target="_blank" rel="noopener noreferrer" class="footer__link-item">MDX Official Docs<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Developer Sites</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://google.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Google<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/ShangjinTang/site" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/site/contact-me">Contact Me</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 by Shangjin Tang</div></div></div></footer></div>
</body>
</html>